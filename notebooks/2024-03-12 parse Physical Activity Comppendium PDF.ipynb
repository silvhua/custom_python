{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\silvh\\OneDrive\\lighthouse\\custom_python\")\n",
    "# sys.path.append(r'C:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\GHL-chat\\src\\app')\n",
    "from silvhua import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the following files:\n",
      "1_2024-adult-compendium_1_2024.pdf\n",
      "An error occurred on line 26 in C:\\Users\\silvh\\AppData\\Local\\Temp\\ipykernel_6156\\2345812797.py: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabula import read_pdf\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "def parse_pdf_tolist(\n",
    "    pdf_filenames, \n",
    "    filepath=r'C:\\Users\\silvh\\OneDrive\\Ginkgo\\references', \n",
    "    pages='all', \n",
    "    # last_id_column='Major Heading', value_to_ignore=None,\n",
    "    # blank_columns=None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create dataframe from a PDF file using \n",
    "    \"\"\"\n",
    "    print(f'Extracting the following files:')\n",
    "    if type(pdf_filenames) != list:\n",
    "        pdf_filenames = [pdf_filenames]\n",
    "    dfs_list = []\n",
    "    reference_dict = dict()\n",
    "    corrected_pages = dict()\n",
    "    for index, file in enumerate(pdf_filenames):\n",
    "        print(file)\n",
    "        filepath = f'{filepath}/'.replace('\\\\','/') if filepath else ''\n",
    "        path_and_file = filepath+file\n",
    "        try:\n",
    "            df_list = read_pdf(path_and_file, pages=pages)\n",
    "            dfs_list.append(df_list)\n",
    "        except Exception as error:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            message = f'An error occurred on line {lineno} in {filename}: {error}.'\n",
    "            print(message)\n",
    "    return dfs_list\n",
    "        # file_dfs = []\n",
    "        # page1_columns = df_list[0].columns.str.replace('\\r', '').str.replace(r'(workplace)([a-zA-Z]+)', r'\\1_\\2', regex=True).to_list()\n",
    "        # print('number of columns:', len(page1_columns))\n",
    "        # if last_id_column in page1_columns:\n",
    "        #     page1_columns_sorted = sorted(page1_columns)\n",
    "        # else:\n",
    "        #     previous_columns = dfs_list[index-1].columns\n",
    "        #     print(f'\\t\\tNo headers found; using headers from previous document:', previous_columns)\n",
    "        #     if len(df_list[page_number].columns) > len(previous_columns):\n",
    "        #         n_cols_to_pad = len(df_list[page_number].columns) - len(previous_columns)\n",
    "        #         new_columns_padding = [f'blank{index}' for index, col in enumerate(range(n_cols_to_pad))]\n",
    "        #         page1_columns_sorted = sorted(page1_columns) + new_columns_padding\n",
    "        #         page1_columns = page1_columns + new_columns_padding\n",
    "\n",
    "        #     elif len(df_list[page_number].columns) < len(previous_columns):\n",
    "        #         page1_columns = (previous_columns[:len(df_list[page_number].columns)].to_list())\n",
    "        #         page1_columns_sorted = sorted(page1_columns)\n",
    "        #     else:\n",
    "        #         page1_columns_sorted = (dfs_list[index-1].columns.to_list())\n",
    "        #     page1_columns = ['Unnamed: 0' if column=='index' else column for column in page1_columns]\n",
    "        #     print(f'\\t\\t**Current df columns:',page1_columns)\n",
    "        \n",
    "        # for page_number in range(len(df_list)):\n",
    "        #     print(f'\\tpage {page_number+1}...')\n",
    "        #     df_list[page_number].columns = df_list[page_number].columns.str.replace('\\r', '').str.replace(r'(workplace)([a-zA-Z]+)', r'\\1_\\2', regex=True)\n",
    "        #     print('\\t\\tCurrent number of columns:', len(df_list[page_number].columns), len(page1_columns_sorted))\n",
    "        #     print(f'\\t\\t{len(page1_columns_sorted)}, {len(page1_columns)}')\n",
    "        #     if len(df_list[page_number].columns.to_list()) == len(page1_columns_sorted):\n",
    "        #         if sorted(df_list[page_number].columns.to_list()) == page1_columns_sorted:\n",
    "        #             pass\n",
    "        #         else:\n",
    "        #             df_list[page_number].columns = page1_columns \n",
    "        #             print('\\t\\tKeeping page 1 columns')\n",
    "        #     else:\n",
    "        #         if len(df_list[page_number].columns) > len(page1_columns_sorted):\n",
    "        #             n_cols_to_pad = len(df_list[page_number].columns) - len(page1_columns_sorted)\n",
    "        #             new_columns_padding = [f'blank{index}' for index, col in enumerate(range(n_cols_to_pad))]\n",
    "        #             df_list[page_number].columns = page1_columns + new_columns_padding\n",
    "        #             print('\\t\\tPadding columns')\n",
    "        #         elif len(df_list[page_number].columns) < len(page1_columns_sorted):\n",
    "        #             df_list[page_number].columns = page1_columns[:len(df_list[page_number].columns)]\n",
    "        #             print('\\t\\tDropping columns')\n",
    "        #         else:\n",
    "        #             df_list[page_number].columns = page1_columns\n",
    "        #         corrected_pages[file] = page_number+1\n",
    "        #         print(f'\\n\\t**Fixing column parsing for page {page_number+1} of {file}**')\n",
    "        #         print(f'\\t\\t{df_list[page_number].columns}')\n",
    "        #     # Consolidate rows that were incorrectly split\n",
    "        #     if (df_list[page_number]['Unnamed: 0'].isna().sum() > 0):\n",
    "        #         # print('\\tAttempting to consolidate rows that were incorrectly split: ', df_list[page_number].columns)\n",
    "        #         print('\\tAttempting to consolidate rows that were incorrectly split: ')\n",
    "        #         try:\n",
    "        #             df_list[page_number] = merge_rows(df_list[page_number], file)\n",
    "        #         except:\n",
    "        #             print(f'\\t\\tUnable to consolidate rows on page {page_number+1} of {file}')\n",
    "        #             reference_dict['unparsed documents'] = file\n",
    "        #             continue\n",
    "\n",
    "        #     # Align the columns\n",
    "        #     if value_to_ignore in df_list[page_number].iloc[0].values:\n",
    "        #         value_to_ignore_column = df_list[page_number].iloc[0].values.tolist().index(value_to_ignore)\n",
    "        #         print(f'\\tRemoving {value_to_ignore} from column {value_to_ignore_column}')\n",
    "        #         df_list[page_number].iloc[0, value_to_ignore_column:-1] = df_list[page_number].iloc[0, value_to_ignore_column+1:].values\n",
    "\n",
    "        #     for column in blank_columns:\n",
    "        #         try:\n",
    "        #             if df_list[page_number][column].isna().sum() != len(df_list[page_number]): # 2023-03-30 22:45:\n",
    "        #                 blank_column_index = df_list[page_number].columns.tolist().index(column)\n",
    "        #                 print(f'\\tShifting data from column {blank_column_index}({column})')\n",
    "        #                 df_list[page_number].iloc[:,blank_column_index+1:] = df_list[page_number].iloc[:,blank_column_index:-1].values \n",
    "        #                 df_list[page_number].iloc[:,blank_column_index] = None\n",
    "        #         except:\n",
    "        #             pass\n",
    "\n",
    "        #     file_dfs.append(df_list[page_number]) # 2023-03-30 23:04 Decrease indent\n",
    "        # dfs_list.append(file_dfs) # SH 2023-04-01 20:58\n",
    "\n",
    "pdf_filenames = ['1_2024-adult-compendium_1_2024.pdf']\n",
    "\n",
    "dfs_list = parse_pdf_tolist(\n",
    "    pdf_filenames, \n",
    "    filepath=r'C:\\Users\\silvh\\OneDrive\\Ginkgo\\references', \n",
    "    pages='all')\n",
    "dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
